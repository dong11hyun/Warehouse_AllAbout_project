## 1. Project Background & Previous Architecture

초기 프로젝트(v1.0)는 **Kakao Local API**를 활용하여 특정 지점(예: 다이소) 주변의 상권 데이터를 실시간으로 수집하고 시각화하는 것에 초점을 맞췄습니다.  
사용자가 요청할 때마다 외부 API를 호출하고, 응답받은 **소량의 데이터**를 PostGIS에 저장하는 방식이었습니다.

---

## 2. Technical Challenge: The Limitations of API-Based Collection

프로젝트의 목표가 **“단순 위치 표시”**에서 **“상권 밀집도 분석 및 거시적 트렌드 파악”**으로 고도화되면서, 기존 API 폴링(Polling) 방식은 명확한 **세 가지 한계점(Bottlenecks)**에 직면했습니다.

### 🛑 1. 데이터 수집의 한계 (Rate Limiting & Pagination)

- **문제점**  
  Kakao API는 1회 요청 시 최대 45개(15 items × 3 pages)의 데이터만 반환합니다.

- **현상**  
  서울시 전체 편의점 분포나 도로망 데이터를 확보하려면 수천 번의 HTTP 요청이 필요하며,  
  이는 API 일일 쿼터 제한(Quota Limit) 도달 또는 수집 속도 저하를 유발합니다.

- **결과**  
  “홍대 주변”과 같은 마이크로 분석은 가능하지만,  
  “서울시 전체”와 같은 매크로 분석에는 구조적으로 부적합했습니다.

---

### 🛑 2. 시계열 및 과거 데이터 부재 (Lack of Historical Data)

- **문제점**  
  API는 호출 시점의 **현재(Snapshot)** 데이터만 제공합니다.

- **현상**  
  “지난달 대비 유동 인구 변화” 또는  
  “특정 시간대의 교통 체증 패턴”과 같은 시계열 분석이 불가능합니다.

- **결과**  
  데이터가 정적(Static)이며,  
  인사이트 도출을 위한 트렌드 분석에 한계가 존재했습니다.

---

### 🛑 3. 분석 쿼리의 비효율성 (Query Inefficiency)

- **문제점**  
  외부 API 의존 구조에서는 데이터를 내부 DB로 가져오기 전까지  
  필터링(Filter)이나 집계(Aggregation)를 수행할 수 없습니다.

- **결과**  
  불필요한 데이터까지 모두 네트워크를 통해 수신해야 하며,  
  대역폭 낭비 및 처리 지연(Latency)이 발생합니다.

---

## 3. Solution: Integrating Google BigQuery Public Datasets

위 한계를 극복하기 위해 **Google BigQuery Public Datasets**를 도입하여  
전체 아키텍처를 재설계했습니다.

BigQuery는 페타바이트(PB)급 데이터를 수 초 내로 처리할 수 있으며,  
OpenStreetMap(OSM), 교통량 데이터 등 대규모 공개 데이터를 **SQL 기반으로 즉시 조회**할 수 있습니다.

### ✅ Key Improvements

- **Bulk Data Processing**  
  API 페이지네이션 없이 SQL 쿼리 한 번으로  
  수백만 건 이상의 지리 데이터(Polygon, LineString)를 추출할 수 있습니다.

- **Advanced Spatial Analysis**  
  단순 반경 검색을 넘어,  
  BigQuery 단계에서 사전 전처리(Pre-processing) 및 필터링을 수행하여  
  백엔드로는 최소한의 정제된 데이터만 전달합니다.

- **Scalability**  
  분석 범위를 도시 단위에서 국가 단위로 확장하더라도  
  아키텍처 변경 없이 대응 가능합니다.

---

## 4. New Data Pipeline Architecture (v2.0)

기존의 **동기식 API 호출 중심 구조**를  
**ETL(Extract, Transform, Load) 파이프라인** 형태로 고도화했습니다.

### 🛠️ Process Flow

- **Extract (BigQuery)**  
  `bigquery-public-data.geo_openstreetmap` 등의 테이블에서  
  분석 대상 지역(예: Seoul)의 건물, 도로 데이터를 SQL로 추출합니다.

- **Transform (Python)**  
  추출된 데이터를 Pandas / GeoPandas로 처리하여  
  결측치 정제 및 **H3 (Uber Hexagonal Index)** 변환을 수행합니다.

- **Load (PostGIS)**  
  전처리가 완료된 데이터를 PostGIS에 적재하고  
  공간 인덱스(GiST)를 생성하여 조회 성능을 최적화합니다.

- **Serve (Django)**  
  프론트엔드 요청 시 복잡한 연산 없이  
  PostGIS의 공간 쿼리를 통해 즉각적인 응답을 제공합니다.

---

## 📝 Conclusion

본 프로젝트의 아키텍처 변화는 단순한 기술 스택 교체가 아니라,  
**“외부 API 의존적인 서비스”**에서  
**“데이터 주권(Data Sovereignty)을 보유한 분석 플랫폼”**으로의 진화를 의미합니다.

이를 통해 대용량 공간 데이터 처리 역량을 확보하였으며,  
보다 정교하고 확장 가능한 상권 분석 서비스를 구현할 수 있었습니다.
