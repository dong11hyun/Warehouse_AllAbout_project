##  v2.1 확장성 고려 및 아키텍처 진화

> **"프로젝트가 '서울시 전체'로 확장될 때, 우리는 어떤 문제를 마주했는가?"**

 v1.0에서 v2.1로 넘어가면서 직면한 **확장성(Scalability)** 문제와 이를 해결하기 위한 **기술적 의사결정(Technical Decision Making)** 과정을 기록했습니다.

---

## 1. 기존 아키텍처(v1.0)의 한계

초기 프로젝트는 사용자가 요청할 때마다 Kakao API를 호출하는 **On-Demand** 방식이었습니다. 하지만 분석 범위가 "강남역"에서 "서울시 전체"로 확장되자, 명확한 한계점들이 드러났습니다.

### 🔺 Bottleneck 1: 데이터 수집의 물리적 한계
- **Rate Limit**: Kakao API는 1회 호출 시 **최대 45개** (15개 x 3페이지) 데이터만 제공합니다.
- **Problem**: 서울시 전체 편의점은 수천 개입니다. 이를 수집하려면 수백 번의 API 호출이 필요하며, 이는 **속도 저하**와 **Quota 제한** 위험을 초래합니다.

### 🔺 Bottleneck 2: 거시적 분석 불가
- **Snapshot Data**: API는 호출 시점의 데이터만 줍니다. "지난달과 비교해서 상권이 어떻게 변했나?" 같은 **시계열 분석**이 불가능합니다.
- **Lack of Insight**: 단순 위치 표시는 가능하지만, "서울시 편의점 밀집도 Heatmap" 같은 **거시적 통계**를 내기 어렵습니다.

---

## 2. 확장성 확보를 위한 첫 번째 시도: Google BigQuery (OSM)

이 문제를 해결하기 위해, 우리는 **"API를 버리고, 이미 모아진 데이터를 쓰자"**는 아이디어를 냈습니다.

### Idea: "전 세계 데이터를 쿼리 한 번으로?"
**Google BigQuery**에 있는 **OpenStreetMap(OSM)** 퍼블릭 데이터셋을 활용하면, 서울시 전체 데이터를 단 몇 초 만에 가져올 수 있을 것이라 기대했습니다.

### 기대했던 아키텍처 (Proposed v2.0)
```
    A[Use Request] -->|No API Call| B(Local DB);
    subgraph Data Pipeline
    C[Google BigQuery (OSM)] -->|Bulk Export| D[Python ETL];
    D -->|Coordinate Transfrom| B;
    end
```

- **장점 1 (Bulk Processing)**: API 페이지네이션 없이 SQL 쿼리 한 큐에 수만 건 수집 가능.
- **장점 2 (Scalability)**: 서울뿐만 아니라 도쿄, 뉴욕 등 전 세계로 즉시 확장 가능.

---

## 3. 현실의 벽: 데이터 품질 (Data Quality)

하지만 실제로 빅쿼리를 돌려보니 치명적인 문제가 발견되었습니다.

| 비교 항목 | PostGIS (Kakao API 기반) | BigQuery (OSM 기반) |
|---|---|---|
| **서울시 다이소 수** | **약 260개** (정확함) | **약 60개** (누락 심각) |
| **편의점 데이터** | 골목 상권까지 커버 | 대로변 위주, 소규모 점포 누락 |
| **결론** | **Adopt (채택)** | **Drop (기각)** |

> **"도구(Tool)가 아무리 좋아도, 원천 데이터(Source Data)가 좋지 않으면 무용지물이다."**

결국 BigQuery는 **확장성**은 좋지만 **정확성**이 떨어져, 상권 분석용으로는 부적합하다는 결론을 내렸습니다.

---

## 4. 최종 해결책: "Divide and Conquer" (PostGIS + Advanced API)

우리는 **데이터의 정확도(Kakao)**와 **분석의 확장성(PostGIS)**을 모두 잡는 하이브리드 방식을 택했습니다.

### 🔹 v2.1 Final Architecture

**1. 수집 단계 (Collection): 정교한 분할 정복**
- "서울"을 한 번에 부르지 않고, **25개 자치구**로 쪼개서 호출 (Coverage 확보).
- 반경 검색 시 **4분면(Quadrant)**으로 쪼개서 45개 제한 우회 (Density 확보).

**2. 저장 단계 (Storage): PostGIS 활용**
- API로 힘들게 모은 데이터를 휘발시키지 않고 **PostGIS**에 적재.
- 이제 "강남역 반경 1km" 검색 시 API를 부르지 않고, **내 DB에서 SQL `ST_DWithin`**으로 0.01초 만에 응답.

### 🔹 결과 (Impact)
- **정확도**: 카카오의 고품질 데이터 100% 활용.
- **성능**: 초기 수집(ETL)은 느리지만, 실제 서비스(Service) 속도는 Real-time 수준으로 향상.
- **분석**: 내 DB에 데이터가 있으므로 시계열 분석, 밀집도 분석 가능.

---

이러한 과정을 통해, API 래퍼(Wrapper)였던 프로젝트가 **"자체 데이터를 보유한 분석 플랫폼"**으로 진화하게 되었습니다.
